### Introduction
1. Offensive tool for digital artists to protect their work using. 
2. Makes use of [[Data Poisoning]] to protect artists' work being used to train A.I. Models without their consent or without compensation or licensing.
3. Paper: [Link](https://arxiv.org/pdf/2310.13828)
### Working
1. > *"Our work is driven by two key insights. First, while diffusion models are trained on billions of samples, the number of training samples associated with a specific concept or prompt is generally on the order of thousands. This suggests that these models will be vulnerable to prompt-specific poisoning attacks that corrupt a model’s ability to respond to specific targeted prompts. Second, poison samples can be carefully crafted to maximize poison potency to ensure success with very few samples."*
2. *"...a prompt-specific poisoning attack optimized for potency that can completely control the output of a prompt in Stable Diffusion’s newest model (SDXL) with less than 100 poisoned training samples."*
3. 